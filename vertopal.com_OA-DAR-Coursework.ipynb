{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`{r setup, include=FALSE} knitr::opts_chunk$set(echo = TRUE)`\n",
    "\n",
    "#### 1. Statistical learning methods ($10\\%$)\n",
    "\n",
    "For each of parts (a) through (d), indicate whether we would generally\n",
    "expect the performance of a non-parametric statistical learning method\n",
    "to be better or worse than a parametric method. Justify your answer.\n",
    "\n",
    "1.  The number of predictors $p$ is large, and the number of\n",
    "    observations $n$ is small. ($2\\%$)\n",
    "\n",
    "In this scenario, performance of non-parametric methods would generally\n",
    "be better than the parametric methods.\n",
    "\n",
    "Non-parametric statistical learning methods, such as decision trees or\n",
    "k-nearest neighbours do not make definite assumptions about functional\n",
    "form of the relationship between predictors and the target variable.\n",
    "Instead, they are flexible because they learn patterns directly from the\n",
    "data and adapts itself when left unconstrained.\n",
    "\n",
    "On the contrary, parametric methods assume that the relationship between\n",
    "predictors and targets maintain linear relationship. Since the\n",
    "Observation is small, this method can struggle accurately estimate the\n",
    "parameters for complex functions due to insufficient data and give large\n",
    "residuals.\n",
    "\n",
    "1.  The sample size $n$ is large, and the number of predictors $p$ is\n",
    "    also large. ($2\\%$)\n",
    "\n",
    "When both the sample size and the number of predictors is large, the\n",
    "performance of non-parametric methods would be worse than parametric\n",
    "methods.\n",
    "\n",
    "With high-dimensional datasets, non-parametric methods can become\n",
    "computationally rigorous, which may also lead to overfitting. With too\n",
    "many predictors, the modelling will be unable to capture the underlying\n",
    "patterns in the data effectively, thus decreasing the performance. This\n",
    "is also known as the curse of dimensionality.\n",
    "\n",
    "Since parametric models assume the relationship between predictors and\n",
    "targets are linear, it can generate better explainable models in complex\n",
    "scenarios.\n",
    "\n",
    "1.  The sample size $n$ is small, and the relationship between the\n",
    "    predictors and response is highly linear. ($3\\%$)\n",
    "\n",
    "When sample size is small, but predictor-response relationship is highly\n",
    "linear, undoubtably non-parametric methods will underperform compared to\n",
    "parametric methods.\n",
    "\n",
    "With larger samples, non-parametric methods offer more robust estimates\n",
    "as they learn patterns directly from the relationships between\n",
    "predictors and responses. When the sample is smaller, methods have less\n",
    "observations available to accurately estimate the underlying\n",
    "relationships. Therefore, with smaller samples, non-parametric methods\n",
    "generate predictions with high variance.\n",
    "\n",
    "Also, since there is small number of data available, the model will try\n",
    "to capture the noise in the data rather than the true underlying\n",
    "relationship. This causes overfitting, which can indeed create models\n",
    "that perform really well on the training dataset but perform poorly with\n",
    "new, unseen data, making the model less reliable.\n",
    "\n",
    "For linear models, parametric methods can capture the actual\n",
    "relationship between predictors and their responses. This reduces\n",
    "prediction errors.\n",
    "\n",
    "1.  The standard deviation of the error terms,\n",
    "    i.e. $\\sigma = \\textrm{sd}(\\varepsilon)$, is extremely high. ($3\\%$)\n",
    "\n",
    "Non-parametric methods will perform better than parametric for extremely\n",
    "high standard deviations.\n",
    "\n",
    "Cases where the error terms have high standard deviations, it is\n",
    "suggested that the predictor-response relationship is mostly likely to\n",
    "be complex or non-linear. A non-parametric method can flexibly adapt to\n",
    "the training data by capturing the variability in the data, if left\n",
    "unconstrained (no specified functional form). But parametric is linear\n",
    "form, so it is constrained.\n",
    "\n",
    "In some cases, a very high standard deviation may occur due to the\n",
    "presence of outliers in the data distributions. But non-parametric\n",
    "methods are more robust to these outliers because they aren’t influenced\n",
    "by data points that deviate from the normality. On the contrary,\n",
    "parametric methods are sensitive to outliers.\n",
    "\n",
    "#### 2. Linear regression ($20\\%$)\n",
    "\n",
    "This question involves the `Auto` dataset included in the “ISLR”\n",
    "package.\n",
    "\n",
    "1.  Use the `lm()` function to perform a simple linear regression with\n",
    "    `acceleration` as the response and `cylinders` as the predictor. Use\n",
    "    the `summary()` function to print the results. Comment on the\n",
    "    output. For example:\n",
    "\n",
    "``` {r}\n",
    "library(ISLR)\n",
    "data(\"Auto\")\n",
    "\n",
    "# loads Auto datasets\n",
    "lm.fit <- lm(acceleration ~ cylinders, data = Auto)\n",
    "summary(lm.fit)\n",
    "```\n",
    "\n",
    "1.  Is there a relationship between the predictor and the response?\n",
    "    ($3\\%$)\n",
    "\n",
    "The result in the summary suggests that there is a significant\n",
    "statistical relationship between the number of cylinders and\n",
    "acceleration.\n",
    "\n",
    "Here, Pr(\\>\\|t\\|) is the P-value, that is \\<2e-16 meaning it is smaller\n",
    "than 0.05 (very close to zero, but not equal to zero). Thus it rejects\n",
    "null hypothesis, implying that there is an observable linear\n",
    "relationship between the predictor and response.\n",
    "\n",
    "Th RSE is 2.385, which suggests that any prediction on the observed\n",
    "acceleration values based on the number of cylinders would be off by\n",
    "2.385 on average when the least squares line is used.\n",
    "\n",
    "1.  How strong is the relationship between the predictor and the\n",
    "    response? ($3\\%$)\n",
    "\n",
    "R-squared value is 0.2547, which is closer to zero than 1. This means\n",
    "that only about 25% of the response’s variance can be explained by the\n",
    "variation of predictor.Thus, the relationship is moderately strong.\n",
    "\n",
    "1.  Is the relationship between the predictor and the response positive\n",
    "    or negative? ($3\\%$)\n",
    "\n",
    "The predictor coefficient (cylinders) is -0.8163. This Negative value\n",
    "suggests that for every additional cylinder in the added to the engine,\n",
    "the acceleration (response) is expected to decrease by 0.8163 units.\n",
    "\n",
    "1.  What is the predicted `acceleration` associated with an `cylinders`\n",
    "    of 3.0? What are the associated 99% confidence and prediction\n",
    "    intervals? ($3\\%$)\n",
    "\n",
    "``` {r}\n",
    "\n",
    "\n",
    "c_interval <- predict(lm.fit, data.frame(cylinders=(c(3.0))), interval = \"confidence\", level = 0.99)\n",
    "print(c_interval)\n",
    "\n",
    "p_interval <- predict(lm.fit, data.frame(cylinders=(c(3.0))), interval = \"prediction\", level = 0.99)\n",
    "print(p_interval)\n",
    "\n",
    "cat(\"\\n\",\"Predicted acceleration:\", c_interval[1], \"\\n\")\n",
    "cat(\"99% Confidence Interval: from \", c_interval[2], \" to \", c_interval[3], \"\\n\")\n",
    "cat(\"99% Prediction Interval: from \", p_interval[2], \" to \", p_interval[3], \"\\n\")\n",
    "```\n",
    "\n",
    "1.  Plot the response and the predictor. Use the `abline()` function to\n",
    "    display the least squares regression line. ($3\\%$)\n",
    "\n",
    "``` {r}\n",
    "\n",
    "plot(Auto$cylinders, Auto$acceleration, \n",
    "     xlab = \"Cylinders\", ylab = \"Acceleration\", \n",
    "     main = \"Scatterplot of Acceleration vs. Cylinders\")\n",
    "\n",
    "# Add the least squares regression line\n",
    "abline(lm.fit, col = \"blue\")\n",
    "```\n",
    "\n",
    "1.  Plot the 99% confidence interval and prediction interval in the same\n",
    "    plot as (b) using different colours and legends. ($5\\%$)\n",
    "\n",
    "``` {r}\n",
    "c_int <- predict(lm.fit, data.frame(cylinders =(Auto$cylinders)), interval = \"confidence\", level = 0.99)\n",
    "p_int <- predict(lm.fit, data.frame(cylinders = (Auto$cylinders)), interval = \"prediction\", level = 0.99)\n",
    "\n",
    "plot(Auto$cylinders, Auto$acceleration, \n",
    "     xlab = \"Cylinders\", ylab = \"Acceleration\", \n",
    "     main = \"Scatterplot of Acceleration vs. Cylinders\")\n",
    "\n",
    "# Add the least squares regression line\n",
    "abline(lm.fit, col = \"blue\")\n",
    "\n",
    "# Add the confidence interval to the plot\n",
    "lines(Auto$cylinders, c_int[,2], col = \"red\", type=\"b\", pch=\"+\")  \n",
    "lines(Auto$cylinders, c_int[,3], col = \"red\", type=\"b\", pch=\"+\")  \n",
    "\n",
    "# Add the prediction interval to the plot\n",
    "lines(Auto$cylinders, p_int[,2], col = \"green\", type=\"b\", pch=\"*\")  \n",
    "lines(Auto$cylinders, p_int[,3], col = \"green\", type=\"b\", pch=\"*\")  \n",
    "\n",
    "\n",
    "# Add legends\n",
    "legend(\"bottomright\",\n",
    "pch=c(\"+\",\"*\"),\n",
    "col=c(\"red\",\"green\"),\n",
    "legend = c(\"confidence\",\"prediction\"))\n",
    "```\n",
    "\n",
    "#### 3. Bayesian networks and naïve Bayes classifiers.($30\\%$)\n",
    "\n",
    "1.  Given a training dataset including 30 observations and a Bayesian\n",
    "    network indicating the relationships between 3 features\n",
    "    (i.e. Income, Student and Credit Rate) and the class attribute\n",
    "    (i.e. Buy Computer), please create the conditional probability\n",
    "    tables by hand. ($10\\%$)\n",
    "\n",
    "2.  Make predictions for 2 testing observations by using a Bayesian\n",
    "    network classifier. ($5\\%$)\n",
    "\n",
    "3.  Based on the conditional independence assumption between features,\n",
    "    please create the conditional probability tables by hand. ($10\\%$)\n",
    "\n",
    "4.  Make predictions for 2 testing observations by using a naïve Bayes\n",
    "    classifier. ($5\\%$)\n",
    "\n",
    "#### 4. Predicting wine quality by using support vector machine classification algorithm. ($40\\%$)\n",
    "\n",
    "1.  Download the full wine quality training and testing datasets from\n",
    "    Moodle, and use the training dataset to find out the optimal value\n",
    "    of hyperparameter C for a linear kernel-based svm. Define the value\n",
    "    of the random seed equals 1 and cost = c(0.01, 1, 100). ($5\\%$)\n",
    "\n",
    "``` {r}\n",
    "\n",
    "#installs library for SVM and cross-validation\n",
    "library(e1071)\n",
    "\n",
    "# loads Training dataset\n",
    "# Converts predictor's categorical values to numerical values (1 for \"Good\" and -1 for \"Bad\")\n",
    "train_WineQuality <- read.table(\"C:/Users/anowe/OneDrive/Documents/WineQuality_Training.txt\", header = TRUE, sep = \",\")\n",
    "train_WineQuality$quality <- ifelse(train_WineQuality$quality == \"Good\", 1, -1)\n",
    "\n",
    "# hyperparameter tuning\n",
    "set.seed(1)\n",
    "cost_val <- c(0.01, 1, 1e2)\n",
    "tune.out <- tune(svm,\n",
    "                quality ~ .,\n",
    "                data = train_WineQuality,\n",
    "                kernel = \"linear\",\n",
    "                ranges = list(cost = cost_val),\n",
    "                )\n",
    "summary(tune.out)\n",
    "```\n",
    "\n",
    "1.  Train a svm classifier by using the linear kernel and the\n",
    "    corresponding optimal value of hyperparameter C, then make\n",
    "    predictions on the testing dataset, report the classification\n",
    "    accuracy. ($10\\%$)\n",
    "\n",
    "``` {r}\n",
    "\n",
    "# converts the target variable from categorical to numerical\n",
    "test_WineQuality <- read.table(\"C:/Users/anowe/OneDrive/Documents/WineQuality_Testing.txt\", header = TRUE, sep = \",\")\n",
    "test_WineQuality$quality <- ifelse(test_WineQuality$quality == \"Good\", 1, -1)\n",
    "\n",
    "# trains SVM model using full training dataset and Optimal value\n",
    "optimal_value_C1 <- 1\n",
    "svmfit.linear.C1 <- svm(quality ~ ., \n",
    "                 data = train_WineQuality,\n",
    "                 type = \"C-classification\",\n",
    "                 kernel = \"linear\", \n",
    "                 cost = optimal_value_C1\n",
    "                 )\n",
    "summary(svmfit.linear.C1)\n",
    "\n",
    "# makes predictions on the test dataset using trained SVM model\n",
    "# prints classification accuracy of predictions in %\n",
    "y.predict.linear.C1 <- predict(svmfit.linear.C1, newdata = test_WineQuality)\n",
    "accuracy_report <- mean(y.predict.linear.C1 == test_WineQuality$quality) * 100\n",
    "print(paste(\"CLASIFICATION ACCURACY REPORT: \", round(accuracy_report, 2), \"%\"))\n",
    "```\n",
    "\n",
    "1.  Use the training dataset to find out the optimal values of\n",
    "    hyperparameters C and for an RBF kernel-based svm. Define the value\n",
    "    of the random seed equals 1, cost = c(0.01, 1, 100) and\n",
    "    gamma=c(0.01, 1, 100). ($5\\%$)\n",
    "\n",
    "``` {r}\n",
    "\n",
    "# Set the random seed\n",
    "# Define the values of C and gamma to try\n",
    "set.seed(1)\n",
    "cost_v <- c(0.01, 1, 100)\n",
    "gamma_v <- c(0.01, 1, 100)\n",
    "\n",
    "# hyperparameter tuning using C and gamma\n",
    "tune.out_C_gamma <- tune(svm,\n",
    "                quality ~ .,\n",
    "                data = train_WineQuality,\n",
    "                kernel = \"radial\",\n",
    "                ranges = list(cost = cost_v, gamma = gamma_v)\n",
    "                )\n",
    "summary(tune.out_C_gamma)\n",
    "```\n",
    "\n",
    "1.  Train a svm classifier by using the RBF kernel and the corresponding\n",
    "    optimal values of hyperparameters C and gamma, then make predictions\n",
    "    on the testing dataset, report the classification accuracy. ($10\\%$)\n",
    "\n",
    "``` {r}\n",
    "# trains SVM model using full training dataset and Optimal value\n",
    "optimal_C <- 100\n",
    "optimal_G <- 1\n",
    "svmfit.radial.C100G1 <- svm(quality ~ ., \n",
    "                 data = train_WineQuality, \n",
    "                 kernel = \"radial\", \n",
    "                 type = \"C-classification\",\n",
    "                 cost = optimal_C,\n",
    "                 gamma = optimal_G,\n",
    "                 probability = TRUE\n",
    "                 )\n",
    "summary(svmfit.radial.C100G1)\n",
    "\n",
    "# makes predictions on the test dataset using trained SVM model\n",
    "# gets classification accuracy of predictions in %\n",
    "y.predict.radial.C100G1 <- predict(svmfit.radial.C100G1, newdata = test_WineQuality)\n",
    "accuracy_report2 <- mean(y.predict.radial.C100G1 == test_WineQuality$quality) * 100\n",
    "print(paste(\"CLASSIFICATION ACCURACY REPORT: \", round(accuracy_report2, 2), \"%\"))\n",
    "```\n",
    "\n",
    "1.  Train a logistic regression model. Then use the testing dataset to\n",
    "    conduct an ROC curve analysis to compare the predictive performance\n",
    "    of the trained logistic regression model and those two svm\n",
    "    classifiers trained by using linear and RBF kernels respectively.\n",
    "    ($10\\%$)\n",
    "\n",
    "``` {r}\n",
    "library(ROCR)\n",
    "\n",
    "train_data = read.table(\"C:/Users/anowe/OneDrive/Documents/WineQuality_Testing.txt\", header = TRUE, sep = \",\")\n",
    "# Convert quality to binary\n",
    "train_data$quality <- ifelse(train_data$quality == \"Good\", 1, 0)\n",
    "\n",
    "# Train logistic regression model\n",
    "train.logit_model <- glm(quality ~ ., \n",
    "                         data = train_data, \n",
    "                         family = binomial,\n",
    "                         maxit = 1000\n",
    "                         )\n",
    "summary(train.logit_model)\n",
    "```\n",
    "\n",
    "``` {r}\n",
    "library(ROCR)\n",
    "\n",
    "test_data = read.table(\"C:/Users/anowe/OneDrive/Documents/WineQuality_Testing.txt\", header = TRUE, sep = \",\")\n",
    "# Convert to binary classification (good vs. not good) for logistic regression\n",
    "test_data$quality <- ifelse(test_data$quality == \"Good\", 1, 0)\n",
    "# Check the structure of predicted_linear\n",
    "\n",
    "# Logistic Regression\n",
    "y.predict.logit <- predict(train.logit_model, newdata = test_data, type = \"response\")\n",
    "p_log <- prediction(y.predict.logit, test_data$quality)\n",
    "# ROC curve and AUC calculation for Logistic Regression\n",
    "perf_logit <- performance(p_log, \"tpr\", \"fpr\")\n",
    "auc_logit <- performance(p_log, \"auc\")\n",
    "```"
   ],
   "id": "42a14256-91c8-446e-92a7-ab6bf11055d3"
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
